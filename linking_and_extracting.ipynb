{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2e7b1e7-1aba-4286-ac26-057e6dcff8d1",
   "metadata": {},
   "source": [
    "# Load Unannotated data from Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "617c536d-435e-4943-a711-9dc924abf899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pandas import json_normalize \n",
    "import json\n",
    "import ast\n",
    "\n",
    "# Read in JSON file and set index to 'id'\n",
    "df=pd.read_json('cal_data.jsonl', lines=True).set_index('id')\n",
    "\n",
    "# Convert dataframe to appropriate data types\n",
    "df=df.convert_dtypes()\n",
    "\n",
    "# Convert 'decision_date' column to datetime and extract date only\n",
    "df['decision_date'] = df['decision_date'].apply(pd.to_datetime).dt.date\n",
    "\n",
    "# Extract nested 'casebody' data and merge with original dataframe\n",
    "xx= df[\"casebody\"].apply(pd.Series)\n",
    "xx1 = xx['data'].apply(pd.Series)\n",
    "df = pd.merge(df,xx,on=['id'])\n",
    "df = pd.merge(df,xx1,on=['id'])\n",
    "\n",
    "# Extract 'cite' value from 'citations' column and create new column 'citation1'\n",
    "df['citation1'] = df['citations'].apply(pd.Series)[0].apply(pd.Series)['cite']\n",
    "\n",
    "# Set 'main_id' column to be the index of the dataframe\n",
    "df['main_id'] = df.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c9dae862",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"main_id\",'citation1','opinions']].to_excel(\"full-unlabelled.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e559fc9d-0c81-4020-9fce-bbf40e52d91f",
   "metadata": {},
   "source": [
    "# Load annotated data from Json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8d018b-339e-4f04-88b0-1d6b753c5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pandas import json_normalize \n",
    "import json\n",
    "import ast\n",
    "# Suppress output from code cell\n",
    "\n",
    "\n",
    "# Specify names of JSON files to load data from\n",
    "files=['ca1.json','ca2.json','ca3.json','ca4.json','ca5.json']\n",
    "\n",
    "# Create empty dataframe to store concatenated data\n",
    "x = pd.DataFrame()\n",
    "\n",
    "# Loop through each file, load data from it, and concatenate with 'x' dataframe\n",
    "for file in files:\n",
    "    with open(file) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        print(file,len(data))\n",
    "    y = json_normalize(data)\n",
    "    # print(file,y.shape)\n",
    "    x = pd.concat([x,y])\n",
    "\n",
    "# Drop duplicate rows based on specific columns and drop the 'index' column\n",
    "x=x.drop_duplicates(subset=['CaseSummary.currentName','CaseSummary.currentLevel','citation'])\n",
    "x=x.drop(['index'],axis=1)\n",
    "\n",
    "# Reset the 'index' column to be sequential\n",
    "x.reset_index(inplace=True)\n",
    "x['index']=x.index\n",
    "\n",
    "# Select only rows where 'History.priorHistoryCount' column is >= 1\n",
    "df_anno_parent=x[x['History.priorHistoryCount'] >= 1]\n",
    "\n",
    "# Initialize an empty dataframe to store processed historical data\n",
    "df_anno_history = pd.DataFrame()\n",
    "\n",
    "# Loop through each row in 'df_anno_parent' dataframe and extract historical data\n",
    "for l in df_anno_parent.iloc:\n",
    "    # Extract index of current row\n",
    "    savel = l['index']\n",
    "    # Extract historical data from 'History.priorHistoryCases' column and store in new dataframe\n",
    "    new = pd.DataFrame(ast.literal_eval(l['History.priorHistoryCases']))\n",
    "    # Rename columns in new dataframe with prefix\n",
    "    new = new.add_prefix('History.priorHistoryCases.')\n",
    "    # Add current row's index to new dataframe\n",
    "    new['index']=l['index']\n",
    "    # Concatenate new dataframe with 'df_anno_history'\n",
    "    df_anno_history = pd.concat([df_anno_history,new])\n",
    "\n",
    "# Reset 'index' column to be sequential\n",
    "df_anno_history.reset_index(inplace=True)\n",
    "df_anno_history.drop([\"level_0\"],axis=1,inplace=True)\n",
    "\n",
    "# Split 'docSummary' column into separate columns and concatenate with 'df_anno_history'\n",
    "df_anno_history=pd.concat([df_anno_history,df_anno_history['History.priorHistoryCases.docSummary'].apply(pd.Series).add_prefix('History.priorHistoryCases.docSummary.')],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "cdecd387-3778-4bf9-9450-afe1b4c614f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to remove values containing the substring 'LEXIS' from a comma-separated string\n",
    "def remove_lexis(x):\n",
    "    # Split the string into a list of values\n",
    "    z = x.split(\",\")\n",
    "    # Create a new list to hold cleaned values\n",
    "    znew = []\n",
    "    # Loop over each value in the original list\n",
    "    for zz in z:\n",
    "        # Check if the value contains 'LEXIS'\n",
    "        # if zz.find('LEXIS') == -1:\n",
    "        #     # If it doesn't, add it to the cleaned list\n",
    "        znew.append(zz)\n",
    "    # Return the cleaned list of values\n",
    "    return znew\n",
    "\n",
    "\n",
    "# Remove rows in 'df_anno_history' where the 'parallels' column is null\n",
    "df_anno_history = df_anno_history[df_anno_history['History.priorHistoryCases.docSummary.parallels'].notna()]\n",
    "\n",
    "# Apply the 'remove_lexis' function to the 'parallels' column to create a new column with cleaned values\n",
    "df_anno_history['parallels_cite'] = df_anno_history['History.priorHistoryCases.docSummary.parallels'].apply(remove_lexis)\n",
    "\n",
    "# Convert the dataframe to a more efficient datatype\n",
    "df_anno_history = df_anno_history.convert_dtypes()\n",
    "\n",
    "# Remove rows where the 'parallels' column is an empty list\n",
    "df_anno_history = df_anno_history[df_anno_history['History.priorHistoryCases.docSummary.parallels'] != \"[]\"]\n",
    "\n",
    "# Add a prefix to the column names in 'df_anno_parent'\n",
    "df_anno_parent = df_anno_parent.add_prefix(\"Anno_parent_\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "d6e77a3f-c264-44fd-92b1-87a390a3b295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_para(x):\n",
    "    l = []\n",
    "    for zz in x:\n",
    "#        if zz['parallelLabel'].find('LEXIS') == -1:\n",
    "        l.append(zz['parallelLabel'])\n",
    "    return l\n",
    "df_anno_parent['Anno_parent_parellels_cite']=df_anno_parent['Anno_parent_CaseSummary.parallelIDs'].apply(extract_para)\n",
    "df_anno_parent['num_par']=df_anno_parent['Anno_parent_parellels_cite'].apply(lambda x: len(x))\n",
    "# df_anno_parent.drop(['Anno_parent_shepID','Anno_parent_itemID','Anno_parent_CaseSummary.shepardsIdentifiers','Anno_parent_CaseSummary.ruriLink',\n",
    "#'Anno_parent_CaseSummary.docFullPath','Anno_parent_CaseSummary.componentID','Anno_parent_CaseSummary.parallelIDs','Anno_parent_CaseSummary.parallelIDsDef',\n",
    "#'Anno_parent_CaseSummary.toplineCategoryCode','Anno_parent_CaseSummary.currentCourt','Anno_parent_CaseSummary.currentJurisdiction','Anno_parent_CaseSummary.courtPath.6',\n",
    "#'Anno_parent_CaseSummary.currentLevel','Anno_parent_History.historyTotal','Anno_parent_History.historyTotalDef','Anno_parent_History.citingDecisions',\n",
    "#'Anno_parent_History.citationDecisionsDef','Anno_parent_History.otherCitingSources','Anno_parent_History.citingDecisionsDef','Anno_parent_History.toaCount',\n",
    "#'Anno_parent_History.toaCountDef','Anno_parent_History.historySummary','Anno_parent_History.historySummaryDef','Anno_parent_History.overrulingRiskCount',\n",
    "#'Anno_parent_History.subsequentAppeals','Anno_parent_History.priorHistory','Anno_parent_History.priorHistoryCases','Anno_parent_CaseSummary.courtPath.5',\n",
    "#'Anno_parent_CaseSummary.courtPath.1','Anno_parent_CaseSummary.courtPath.2','Anno_parent_CaseSummary.courtPath.3','Anno_parent_CaseSummary.courtPath.4',\n",
    "#'Anno_parent_CaseSummary.courtPath.7','Anno_parent_CaseSummary.courtPath.8'],axis=1,inplace=True)\n",
    "\n",
    "df_anno_parent['Anno_parent_citation1'] = df_anno_parent[df_anno_parent['Anno_parent_citation'].str.find('LEXIS') != -1]['Anno_parent_parellels_cite'].apply((lambda x: x[0] if len(x)>0 else None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ccee7bc8-00c3-408c-8938-ae5ddb68b3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uni = pd.DataFrame()\n",
    "uni = []\n",
    "uni_set = []\n",
    "cites = df['citation1'].unique()\n",
    "for ind in list(df_anno_history['index'].unique()):\n",
    "    df_ind = df_anno_history[df_anno_history['index']==ind]\n",
    "    df_ind_parent = df_anno_parent[df_anno_parent['Anno_parent_index']==ind]\n",
    "    newset = set()\n",
    "    for ll in df_ind['parallels_cite'].iloc:\n",
    "        for lll in ll:\n",
    "            if lll.lstrip() in cites:\n",
    "                newset.add(lll.lstrip())\n",
    "    for ll in df_ind_parent['Anno_parent_parellels_cite'].iloc:\n",
    "        newset.add(lll)\n",
    "    uni_set.append([ind,list(newset)])\n",
    "    for see in list(newset):\n",
    "        uni.append([ind,see])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "e5ee6070-b642-488b-b839-e3df836af6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uni = pd.DataFrame(uni,columns = [\"index\",\"uniq_history\"])\n",
    "\n",
    "df_uni_list = pd.DataFrame(uni_set,columns = [\"index\",\"uniq_history_list\"])\n",
    "\n",
    "df_anno_parent = pd.merge(df_anno_parent,df_uni_list,left_on=[\"Anno_parent_index\"],right_on=[\"index\"])\n",
    "\n",
    "unanno_existing_cases = pd.merge(df_uni,df,left_on=[\"uniq_history\"],right_on=[\"citation1\"])\n",
    "\n",
    "# df_anno_parent_main_id = pd.merge(df_anno_parent,df,left_on=['Anno_parent_parellels_cite'],right_on=[\"citation1\"])\n",
    "\n",
    "# df_anno_parent_main_id.to_excel(\"df_anno_parent_with_main.xlsx\")\n",
    "\n",
    "# df_anno_parent.to_excel(\"df_anno_parent.xlsx\")\n",
    "\n",
    "# df_anno_parent.columns\n",
    "\n",
    "# df_anno_parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "67b44661-fbeb-4fb9-97f5-5e5e28fd8127",
   "metadata": {},
   "outputs": [],
   "source": [
    "anno_existing_cases = pd.merge(df_uni,df_anno_parent.add_prefix('child_'),left_on=[\"uniq_history\"],right_on=['child_Anno_parent_citation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "dadbaa16-7777-4927-acbd-99a999700607",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_side = pd.merge(anno_existing_cases,df_anno_parent,left_on=[\"index\"],right_on=['Anno_parent_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "09e16ec6-a668-4cc0-87f0-d8b7c647f58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "both_side.to_excel(\"both_side.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "fb29983c-478a-475e-8d81-fa189f354cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anno_history.to_excel('history.xlsx')\n",
    "\n",
    "df_anno_parent.to_excel('parent.xlsx')\n",
    "\n",
    "unanno_existing_cases.to_excel('unanno.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
